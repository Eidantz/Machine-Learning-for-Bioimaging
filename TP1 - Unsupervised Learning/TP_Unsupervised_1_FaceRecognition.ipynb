{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TP_Unsupervised_1_FaceRecognition.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKe0P2J2V6fn"
      },
      "source": [
        "## Face recognition\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxoYGZl-VoSK"
      },
      "source": [
        "Load the original images present in the files *'YaleB\\_32x32.mat'*. This is a small part of the freely available Extended Yale Face Database B downloaded from http://www.cad.zju.edu.cn/home/dengcai/Data/FaceData.html. It contains 2414 cropped images resized to 32x32 pixels. Every image is represented as a vector 1x1024 and all images are stacked in a matrix called data. There are 38 subjects with around 64 near frontal images per individual under different illumination conditions. Once loaded and normalised the data, such that the pixels are between 0 and 1, you can plot images using the function *'imshow'*.\n",
        "\n",
        "# Goal\n",
        "The goal of this part is to evaluate the performance of the dimensionality reduction techniques presented this morning for face recognition. We divide the data-set into two parts, training and test. For every dimensionality reduction technique, you will first extract a set of basis images from your training data-set. Then, you will project the test subjects in this new basis and use the nearest neighbor algorithm to evaluate the performance of the dimensionality reduction technique. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "al-VlsavWVw6"
      },
      "source": [
        "Let's load the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rhu_TEsRA5BW"
      },
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "  from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "  gdd.download_file_from_google_drive(file_id='1rgICXtcIAgDqSoHnNXNZMD_iNABF3RZA',\n",
        "  dest_path='./YaleB_32x32.mat')\n",
        "else:\n",
        "  print('You are not using Colab. Please define working_dir with the absolute path to the folder where you downloaded the data')\n",
        "\n",
        "# Please modify working_dir only if you are using your Anaconda (and not Google Colab)\n",
        "Working_directory=\"./\"   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uY_EMa-KWXrt"
      },
      "source": [
        "Load the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiSmmYCsBP-V"
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import numpy.matlib\n",
        "import matplotlib.pyplot as plt\n",
        "plt.close('all')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.decomposition import KernelPCA\n",
        "from sklearn.decomposition import FastICA\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import NMF\n",
        "\n",
        "\n",
        "from scipy import linalg as LA\n",
        "from scipy.stats import ortho_group\n",
        "\n",
        "from scipy.io import loadmat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mg9lCQSm0N4l"
      },
      "source": [
        "Here you can copy the functions `pcaLecture`, `KpcaGaussianLecture` and `FastICALecture` implemented in the previous jupyter-notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiOg9Rq1XCcS"
      },
      "source": [
        "# For IMP\n",
        "def pcaLecture(X):\n",
        "    XXXXXXX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJNetKfH-Wah"
      },
      "source": [
        "# For IMP\n",
        "def KpcaGaussianLecture(X,gamma):\n",
        "    XXXXXXXX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuOBR8ozXAAQ"
      },
      "source": [
        "# For IMP\n",
        "def FastICALecture(X,N_Iter=3000,tol=1e-5,plot_evolution=0,whiten=True):\n",
        "    XXXXXXX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qG3yNVvKWfYw"
      },
      "source": [
        "This is a useful function to plot the basis images. Be careful, each row of data is a basis image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xe_ty98JBejg"
      },
      "source": [
        "def plotFaces(data,r,c,ncol=2,N=0,indeces=None,title=None):\n",
        "    # data: each face is a row in data\n",
        "    # r,c = number of rows and columns of each image\n",
        "    # n_col = number of columns for subplots\n",
        "    # N = random images to plot (used only if indeces is empty)\n",
        "    # indeces = indeces of images to plot\n",
        "    # title = title of the plot\n",
        "\n",
        "   \n",
        "    if indeces is None:\n",
        "        if N==0:\n",
        "            raise NameError('You should define either N or indeces')\n",
        "        else:\n",
        "            print('Use N random subjects')\n",
        "            indeces=np.random.randint(0,data.shape[0],(N,1))\n",
        "            \n",
        "    nrow=math.ceil(len(indeces)/ncol)\n",
        "    \n",
        "    fig=plt.figure(figsize=(17, 6))\n",
        "    plt.suptitle(title, size=16)\n",
        "    for i, index in enumerate(indeces):\n",
        "        fig.add_subplot(nrow, ncol, i+1)\n",
        "        plt.imshow(np.resize(data[index,:],(r,c)).T,origin='upper',cmap='gray')\n",
        "        plt.xticks(())\n",
        "        plt.yticks(())\n",
        "    #plt.subplots_adjust(left=0.01, bottom=0.05, right=0.99, top=0.93, wspace=0.04, hspace=0.0)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvJ5nT2VWqCF"
      },
      "source": [
        "Let's load the data and compute some parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVfQFQLFBV0w"
      },
      "source": [
        "x = loadmat(Working_directory + './YaleB_32x32.mat')\n",
        "data=x['fea']\n",
        "d=data.shape[1] # number of pixels of the images\n",
        "subjectIndex=x['gnd'] # we have one index per subject\n",
        "maxValue = np.max(np.max(data)) # max intensity value\n",
        "data = data/maxValue; # Scale pixels to [0,1]\n",
        "\n",
        "Ns=len(np.unique(subjectIndex)); # Number subjects\n",
        "Is=round(len(subjectIndex)/Ns) # Number images per subject (on average, not the same number for every subject)\n",
        "r=int(np.sqrt(d)) # number rows of each image\n",
        "c=r # number columns of each image, equal to row since images are square\n",
        "\n",
        "print('There are', data.shape[0], 'facial images and each image has', d, 'pixels' )\n",
        "print('There are', Ns, 'different subjects and each subject has on average', Is, 'images')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtMFoLlsWzrz"
      },
      "source": [
        "Let's plot first 10 images of different subjects and then 10 images of the same subject but with different positions and illumination conditions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BS22jyD6B2lr"
      },
      "source": [
        "# Plot data      \n",
        "indexDifferent=np.arange(1,Is*40,Is)     \n",
        "plotFaces(data,r,c,ncol=3,indeces=indexDifferent[0:10],title='Different subjects')       \n",
        "indexSame=np.arange(0,10,1)      \n",
        "plotFaces(data,r,c,ncol=2,indeces=indexSame,title='Different positions of the same subjects')       \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcId1_oh3Yzx"
      },
      "source": [
        "We can now use PCA to investigate the main variations within the data.\n",
        "\n",
        "**Question:**\n",
        "1. (IMP + IMH) How many modes do you need to explain at least 80% of the variability in the data ? Look at the three main modes and explain which are the main variations in the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0gmxrUps9bb"
      },
      "source": [
        "# Linear interpolation along the first two modes\n",
        "Xm=data.mean(axis=0) # average face\n",
        "\n",
        "pca = PCA(random_state=1) # by fixing the random_state we are sure that results are always the same\n",
        "YpcaTrain=pca.fit_transform(data)\n",
        "UpcaTrain=pca.components_.T # we want PC on columns\n",
        "var_explained_pca=pca.explained_variance_ratio_\n",
        "DpcaTrain = (pca.singular_values_)**2/(data.shape[0]-1) # computation of the eigenvalues\n",
        "indices=np.linspace(-3, 3, num=7, dtype=np.int16) # Interpolation indices\n",
        "\n",
        "# Variance explained by each eigenvector\n",
        "fig=plt.figure(figsize=(7, 7))\n",
        "ax=plt.subplot(111)\n",
        "ax.set_xlim(0, 9)\n",
        "dim=np.arange(0,10,1)\n",
        "plt.plot(np.concatenate(([0], np.cumsum(var_explained_pca)*100)))\n",
        "plt.xticks(dim)\n",
        "plt.xlabel('Number of eigenvectors',fontsize=15)\n",
        "plt.ylabel('% variance explained',fontsize=15)\n",
        "plt.title('Variance explained by PCA modes',fontsize=17)\n",
        "\n",
        "## First mode\n",
        "fig=plt.figure(figsize=(17, 3))\n",
        "plt.suptitle('Variations along the first mode of PCA', size=20)\n",
        "for i, index in enumerate(indices):\n",
        "  image = Xm + index * np.sqrt(DpcaTrain[0]) * UpcaTrain[:,0]\n",
        "  fig.add_subplot(1, len(indices), i+1)\n",
        "  plt.imshow(np.resize(image,(r,c)).T,origin='upper',cmap='gray')\n",
        "  if index != 0:\n",
        "    plt.xlabel(r'%i $\\sigma$' %index, fontsize=15)\n",
        "  else:\n",
        "    plt.xlabel('Average face', fontsize=15)\n",
        "  plt.xticks(())\n",
        "  plt.yticks(())\n",
        "  plt.subplots_adjust(left=0.01, bottom=0.05, right=0.99, top=0.93, wspace=0.04, hspace=0.0)\n",
        "\n",
        "\n",
        "## Second mode\n",
        "fig=plt.figure(figsize=(17, 3))\n",
        "plt.suptitle('Variations along the second mode of PCA', size=20)\n",
        "for i, index in enumerate(indices):\n",
        "  image = Xm + index * np.sqrt(DpcaTrain[1]) * UpcaTrain[:,1]\n",
        "  fig.add_subplot(1, len(indices), i+1)\n",
        "  plt.imshow(np.resize(image,(r,c)).T,origin='upper',cmap='gray')\n",
        "  if index != 0:\n",
        "    plt.xlabel(r'%i $\\sigma$' %index, fontsize=15)\n",
        "  else:\n",
        "    plt.xlabel('Average face', fontsize=15)\n",
        "  plt.xticks(())\n",
        "  plt.yticks(())\n",
        "  plt.subplots_adjust(left=0.01, bottom=0.05, right=0.99, top=0.93, wspace=0.04, hspace=0.0)\n",
        "\n",
        "## Third mode\n",
        "fig=plt.figure(figsize=(17, 3))\n",
        "plt.suptitle('Variations along the third mode of PCA', size=20)\n",
        "for i, index in enumerate(indices):\n",
        "  image = Xm + index * np.sqrt(DpcaTrain[2]) * UpcaTrain[:,2]\n",
        "  fig.add_subplot(1, len(indices), i+1)\n",
        "  plt.imshow(np.resize(image,(r,c)).T,origin='upper',cmap='gray')\n",
        "  if index != 0:\n",
        "    plt.xlabel(r'%i $\\sigma$' %index, fontsize=15)\n",
        "  else:\n",
        "    plt.xlabel('Average face', fontsize=15)\n",
        "  plt.xticks(())\n",
        "  plt.yticks(())\n",
        "  plt.subplots_adjust(left=0.01, bottom=0.05, right=0.99, top=0.93, wspace=0.04, hspace=0.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPn52HoI4P5k"
      },
      "source": [
        "We can now move to evaluate the  performance of the dimensionality reduction techniques presented this morning for face recognition. We first divide the data-set into two parts, training (80%) and test (20%) in a stratified way (subjects are divided in a balanced way between the two parts).\n",
        "\n",
        "We will see in the next lecture why we need to divide into training and test sets. For now, just know that we will use the first set to train our algorithm and the second set to test the performance of our algorithm on new, unseen data.\n",
        "\n",
        "For every dimensionality reduction technique, you will first extract a set of basis images from your training data-set. Then, you will project the test subjects in this new basis and use the nearest neighbor algorithm\n",
        "to evaluate the performance of the dimensionality reduction technique. For each test sample, the nearest neighbor algorithm simply looks for the closest training sample and then assigns the same label (i.e. index of subject).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10xImICJ6RMV"
      },
      "source": [
        "Xtrain, Xtest, Id_Train, Id_Test = train_test_split(data,subjectIndex,test_size=0.20,stratify=subjectIndex, random_state=44)\n",
        "Xctest=Xtest-np.mean(Xtest,axis=0) # centering\n",
        "Xctrain=Xtrain-np.mean(Xtrain,axis=0) # centering"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iQ-llAPfL_y"
      },
      "source": [
        "As first idea, we could simply use the pixel intensities as features. This is basically like using the original data, without dimensionality reducton techniques."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnVJP6T0CUeW"
      },
      "source": [
        "## Use the pixel intensities to find the correct subject for the test images\n",
        "NN=KNeighborsClassifier(n_neighbors=1)\n",
        "NN.fit(Xctrain,Id_Train.ravel())\n",
        "print('By using the pixel intensities, we use ', Xctrain.shape[1], ' features')\n",
        "print('Percentage of correct answer using the pixel intensities is ', NN.score(Xctest,Id_Test))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmYtcEy2XwQ5"
      },
      "source": [
        "# PCA\n",
        "\n",
        "You will first use PCA. Compute the scores $Y_{train}$, eigenvectors $U_{train}$ and eigenvalues $D_{train}$ of the training set. The eigenvectors $U_{train}$ represent the basis images and they are usually called *'Eigenfaces'*. Then, project both training and test data onto the eigenvectors that explain 99$\\%$ of the variability of the training set $L_{train}^{99}$. You will obtain two vectors of scores, $Y_{train}^{99}=X_{train} L_{train}^{99}$ and $Y_{test}^{99}=X_{test} L_{test}^{99}$, which you will use for evaluating the performance of the algorithm. Use the function `KNeighborsClassifier` to test the performance.\n",
        "\n",
        "**Practical Questions:** \n",
        "\n",
        "1. (IMP+IMH) Use either the scikit-learn implementation or yours (better!) to compute the PCA for the training data-set. Comment the eigenfaces. Do they seem \"real\" ?\n",
        "\n",
        "**Theoretical Questions:** \n",
        "\n",
        "1. (IMP+IMH) In your opinion, why do we need to center the data before computing a PCA ? If you want, you can use the previous toy examples to answer this question.\n",
        "2. (IMP+IMH) Let $X$ be the original data, a matrix $\\left[ N,d \\right]$, and $Y$ the scores of a PCA keeping all eigenvectors, which means that $Y$ is also a matrix $\\left[ N,d \\right]$. Are $X$ and $Y$ equal ? If not, why ? What would you use (generally speaking) in a machine learning problem ? Why ?\n",
        "3. (IMP) Let $x_p$ and $x_q$ be two row-vectors representing two images, $U$ an *orthogonal* matrix whose columns are the eigenvectors of $X$ and $y_p=x_pU$, $y_q=x_qU$, check that $x_px_q^T=y_py_q^T$. This shows that $Y=XU$ is a linear transformation that preserves inner products.\n",
        "4. (IMP) Let $C$ be the covariance matrix of $X$ and $C=UDU^T$ its eigen decomposition. Show that the covariance matrix of $Y=XU$ is $D$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDOWexKHn_Ba"
      },
      "source": [
        "## PCA (scikit-learn implementation)\n",
        "pca = PCA(random_state=1) # by fixing the random_state we are sure that results are always the same\n",
        "YpcaTrain=pca.fit_transform(Xtrain)\n",
        "UpcaTrain=pca.components_.T # we want PC on columns\n",
        "var_explained_pca=pca.explained_variance_ratio_\n",
        "\n",
        "# Threshold defined as 99% of the variability\n",
        "Threshold_PCA = 0.99\n",
        "CumulativePca=np.cumsum(var_explained_pca)\n",
        "indexPCA=np.argwhere(CumulativePca>Threshold_PCA)\n",
        "PCAComp=indexPCA[0][0]\n",
        "\n",
        "print('PCA uses ', PCAComp, ' features')\n",
        "\n",
        "# Selection of the eigenvectors \n",
        "Yr_train_PCA=YpcaTrain[:,:PCAComp]\n",
        "Ur_train_PCA=UpcaTrain[:,:PCAComp]\n",
        "\n",
        "# Computation of the test scores using the eigenvectors computed with the\n",
        "# training data-set\n",
        "Yr_test_PCA=np.dot(Xctest,Ur_train_PCA)\n",
        "\n",
        "# Plot the Eigenfaces\n",
        "plotFaces(UpcaTrain.T,r,c,ncol=2,indeces=np.arange(0,10,1),title='PCA - Eigenfaces')       \n",
        "\n",
        "# Score\n",
        "NN.fit(Yr_train_PCA,Id_Train.ravel())\n",
        "print('Percentage of correct answer using PCA is ', NN.score(Yr_test_PCA,Id_Test))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "267H-LajClYe"
      },
      "source": [
        "# For IMP\n",
        "## PCA (your implementation)\n",
        "XXXXXXXXXXX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxhuHs7uFeBl"
      },
      "source": [
        "# KPCA\n",
        "\n",
        "In this section, we are going to do exactly the same procedure as before but using Kernel-PCA with a Gaussian kernel. Remember that we need to compute and center the test kernel matrix $[\\mathbf{\\tilde{K}}]_{ij}=<\\phi(x_i)-\\frac{1}{N} \\sum_{s=1}^N \\phi(x_s),\\phi(x_j)-\\frac{1}{N} \\sum_{s=1}^N \\phi(x_s)>$ and that, once computed the basis vectors in the training set $\\{  \\mathbf{\\alpha}_i \\}$, we can compute the score for a test sample $t$ using the following equation:\n",
        "\n",
        "$y_i(t) = \\sum_{j=1}^N a_{ij} <\\phi(t)-\\frac{1}{N} \\sum_{s=1}^N \\phi(x_s),\\phi(x_j)-\\frac{1}{N} \\sum_{s=1}^N \\phi(x_s)> = \\sum_{j=1}^N a_{ij} \\tilde{k}(t,x_j)$\n",
        "\n",
        "Answer the following questions:\n",
        "\n",
        "**Questions:** \n",
        "\n",
        "1. (IMP+IMH) Look for the best gamma value\n",
        "1. (IMP+IMH) Why the basis vectors $\\{  \\mathbf{\\alpha}_i \\}$ are not plotted as in PCA ?\n",
        "2. (IMP+IMH) Is it worth it, in your opinion, to compute PCA and/or KPCA ? Why not using the original pixel intensities ? Please consider the following aspects in your answer: performance, computational time, number of features, and interpretability of the results.\n",
        "3. (IMP - Optional) Create a new function `Kpca_poly_lecture` where you change the kernel to $k(x,y)=<x,y>^d$. Evaluate the performance of this new kernel.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSBlX410pWh8"
      },
      "source": [
        "# Kernel-PCA (scikit-learn implementation)\n",
        "## choose a gamma value\n",
        "gamma=4\n",
        "#\n",
        "\n",
        "Kpca = KernelPCA(kernel='rbf', gamma=gamma, remove_zero_eig=True, random_state=1)\n",
        "YKpca=Kpca.fit_transform(Xctrain)\n",
        "DKpca=Kpca.lambdas_\n",
        "AnKpca=Kpca.alphas_\n",
        "\n",
        "# variance explained\n",
        "tot=np.sum(np.real(DKpca))\n",
        "varexplKpca = DKpca/tot # computation of explained variance\n",
        "\n",
        "# Threshold defined as 99% of the variability\n",
        "Threshold_KPCA = 0.99\n",
        "CumulativeKPca=np.cumsum(varexplKpca)\n",
        "indexKPCA=np.argwhere(CumulativeKPca>Threshold_KPCA)\n",
        "KPCAComp=indexKPCA[0][0]\n",
        "\n",
        "# Selection of the eigenvectors \n",
        "Yr_train_KPCA=YKpca[:,:KPCAComp]\n",
        "Anr_train_KPCA=AnKpca[:,:KPCAComp]\n",
        "\n",
        "# Construction matrix K for test\n",
        "N = Xctrain.shape[0]\n",
        "M = Xctest.shape[0]\n",
        "InnerX = np.dot(Xctest,Xctrain.T)\n",
        "tempTrain=np.sum(Xctrain**2,axis=1).reshape((1,N))\n",
        "tempTest=np.sum(Xctest**2,axis=1).reshape((M,1))\n",
        "NormTrain2 = np.repeat(tempTrain,M,axis=0)\n",
        "NormTest2 = np.repeat(tempTest,N,axis=1) \n",
        "Norm = NormTest2+NormTrain2-2*InnerX\n",
        "Norm[Norm<1e-10]=0    \n",
        "Ktest=np.exp(-Norm/(2*gamma**2))\n",
        "\n",
        "# Centering kernel test matrix\n",
        "oneN=np.ones((N,N))/N\n",
        "oneM=np.ones((M,M))/M\n",
        "KcTest=Ktest-np.dot(oneM,Ktest)-np.dot(Ktest,oneN) + np.dot(np.dot(oneM,Ktest),oneN) # center kernel matrix\n",
        "       \n",
        "# Computation of the test scores using the eigenvectors computed with the training data-set        \n",
        "Yr_test_KPCA=np.dot(KcTest,Anr_train_KPCA) \n",
        "\n",
        "print('KPCA uses ', Yr_train_KPCA.shape[0], ' features')\n",
        "\n",
        "\n",
        "# Score\n",
        "NN.fit(Yr_train_KPCA,Id_Train.ravel())\n",
        "print('Percentage of correct answer using KPCA is ', NN.score(Yr_test_KPCA,Id_Test.ravel()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFS7m9LD-bkn"
      },
      "source": [
        "# For IMP\n",
        "## KPCA (your implementation)\n",
        "XXXXXXXXXXXXX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkgegzdsdFsm"
      },
      "source": [
        "# ICA\n",
        "\n",
        "In the next section you will evaluate ICA. Every image $x_i$ can be seen as a linear combination of basis images. ICA can be used in two different ways for face recognition. We can look for a set of statistically independent basis images $s_j$ (first architecture) or for a set of statistically independent coefficients $a_{j}$ (second architecture).\n",
        "\n",
        "In the first architecture, we compute $X'=A'S'$, where every row of $X'$ is an image and the columns are pixels. Images are considered as random variables and we look for a set of statistically independent basis images contained in the rows of $S'$.\n",
        "\n",
        "In the second architecture, we transpose the previous setting computing $X''=A''S''$, where every column of $X''$ is an image and rows are pixels. In this case, we consider the pixels as random variables and we look for a set of statistically independent coefficients contained in the rows of $S$ and a set of basis images in the columns of $A$.\n",
        "\n",
        "Instead than using the original training data $X$ as input matrix, we are going to use the eigenvectors (first architecture) or the scores (second architecture) computed with PCA, namely $Y=XL$ (same notation as in the slides of the lecture). In this way, we reduce the computational time since the number of eigenvectors that account for 99\\% of the variance of the training images (columns of $L$) is definitely lower than the number of pixels (columns of $X$). If you want, you can of course use the original data but it will take much more time to converge.\n",
        "\n",
        "For the first architecture we will use $L^T$ as input matrix. In fact, we can notice that the PCA approximation of the matrix $X_{train}$, containing an image in every row, can be written as $\\tilde{X} = YL^T$. If we use $L^T$ as input in the ICA algorithm we obtain $L^T=AS$, thus it follows that $\\tilde{X}=YW^TS$ (since $A=W^{-1}=W^T$). The basis images are contained in the rows of $S$ and the coefficients used for evaluating the performance are instead contained in the rows of $Y_{train}W^T$ for the training set and in $Y_{test}W^T$ for the test set.\n",
        "\n",
        "For the second architecture, we will instead use $Y^T$ as input matrix thus obtaining $Y^T=AS$. Remember that in the second architecture we want to apply the ICA algorithm to the transpose of $X_{train}$, namely $X^T=AS$. We can notice that, given the PCA transformation $Y=XL$, one can write $X \\approx YL^T$ which entails $X^T \\approx LY^T=LAS=LW^TS$. The columns of $LW^T$ contain the basis images whereas the columns of $S$ contain the statistically independent coefficients used to test the performance of the algorithm. The coefficients for the test set are in the columns of $S_{test}=W_{train}Y_{test}^T$.\n",
        "\n",
        "NB: Here we used $X=X_c$ which means centered face images\n",
        "\n",
        "**Questions:**\n",
        " \n",
        "\n",
        "1.   (IMP+IMH) Look at the results of the two architecures. Which one is better ?\n",
        "2.   (IMP+IMH) Looking at the basis images, in which case do they seem more 'real' ?\n",
        "3.   (IMP - Optional) Implement the two architecture using your own imlementation of ICA (`FastICALecture`) instead than the one of scikit-learn (Be careful at the input data...scikit-learn wants a [observations, features] matrix) \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yk2fqwdBasYC"
      },
      "source": [
        "#ICA - first architecture (scikit-learn implementation)\n",
        "pca = PCA(random_state=1) # by fixing the random_state we are sure that results are always the same\n",
        "YpcaTrain=pca.fit_transform(Xtrain)\n",
        "UpcaTrain=pca.components_.T # we want PC on columns\n",
        "var_explained_pca=pca.explained_variance_ratio_\n",
        "\n",
        "# We use the PCA projection to speed up results\n",
        "# Threshold defined as 99% of the variability\n",
        "Threshold_PCA = 0.99\n",
        "CumulativePca=np.cumsum(var_explained_pca)\n",
        "indexPCA=np.argwhere(CumulativePca>Threshold_PCA)\n",
        "PCAComp=indexPCA[0][0]\n",
        "\n",
        "# Selection of the eigenvectors \n",
        "Yr_train_PCA=YpcaTrain[:,:PCAComp]\n",
        "Ur_train_PCA=UpcaTrain[:,:PCAComp]\n",
        "Yr_test_PCA=np.dot(Xctest,Ur_train_PCA)\n",
        "\n",
        "ICA= FastICA(whiten=True, fun='exp', max_iter=30000, algorithm='parallel', tol=1e-4)\n",
        "Yica=ICA.fit_transform(Ur_train_PCA)\n",
        "Sica=Yica.T\n",
        "Aica=ICA.mixing_\n",
        "Wica=ICA.components_\n",
        "\n",
        "Y_test_ICA= np.dot(Yr_test_PCA, Aica)\n",
        "Y_train_ICA = np.dot(Yr_train_PCA,Aica)\n",
        "\n",
        "print('ICA uses ', Y_train_ICA.shape[0], ' features')\n",
        "\n",
        "# Plot the Eigenfaces\n",
        "plotFaces(Sica,r,c,ncol=2,indeces=np.arange(0,10,1),title='ICA - first architecture')  \n",
        "\n",
        "# Score\n",
        "NN.fit(Y_train_ICA,Id_Train.ravel())\n",
        "print('Percentage of correct answer using ICA arch.1 is ', NN.score(Y_test_ICA,Id_Test.ravel()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxt0bRd8QLZr"
      },
      "source": [
        "# For IMP\n",
        "# ICA - First architecture (your implementation)\n",
        "XXXXXXXXXXX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Vs79NwZanFF"
      },
      "source": [
        "## ICA\n",
        "# Second architecture (scikit-learn implementation)\n",
        "pca = PCA(random_state=1) # by fixing the random_state we are sure that results are always the same\n",
        "YpcaTrain=pca.fit_transform(Xtrain)\n",
        "UpcaTrain=pca.components_.T # we want PC on columns\n",
        "var_explained_pca=pca.explained_variance_ratio_\n",
        "\n",
        "# Threshold defined as 99% of the variability \n",
        "Threshold_PCA = 0.99\n",
        "CumulativePca=np.cumsum(var_explained_pca)\n",
        "indexPCA=np.argwhere(CumulativePca>Threshold_PCA)\n",
        "PCAComp=indexPCA[0][0]\n",
        "\n",
        "# Selection of the eigenvectors \n",
        "Yr_train_PCA=YpcaTrain[:,:PCAComp]\n",
        "Ur_train_PCA=UpcaTrain[:,:PCAComp]\n",
        "Yr_test_PCA=np.dot(Xctest,Ur_train_PCA)\n",
        "\n",
        "ICA= FastICA(whiten=True, fun='exp', max_iter=30000, tol=1e-4, algorithm='parallel', random_state=1)\n",
        "Yica=ICA.fit_transform(Yr_train_PCA)\n",
        "S_train_ICA=Yica.T\n",
        "W_train_ICA=ICA.components_\n",
        "\n",
        "ICAFAces=np.dot(Ur_train_PCA,W_train_ICA.T) \n",
        "Y_train_ICA=S_train_ICA\n",
        "Y_test_ICA=np.dot(W_train_ICA,Yr_test_PCA.T)\n",
        "\n",
        "# Plot the ICA-faces\n",
        "plotFaces(ICAFAces.T,r,c,ncol=2,indeces=np.arange(0,10,1),title='ICA-faces')      \n",
        "\n",
        "print('ICA uses ', Y_train_ICA.shape[0], ' features')\n",
        "\n",
        "# Score ICA\n",
        "\n",
        "NN.fit(Y_train_ICA.T,Id_Train.ravel())\n",
        "print('Percentage of correct answer using ICA is ', NN.score(Y_test_ICA.T,Id_Test.ravel()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiUbGQnYUyWG"
      },
      "source": [
        "# For IMP\n",
        "## ICA - second architecture (your implementation)\n",
        "XXXXXXXXXXX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbSo66G1f5Ew"
      },
      "source": [
        "# NNMF\n",
        "\n",
        "Here you will test Non-negative Matrix factorization. The basis images of the training are in the matrix $W_{train}$ and the scores (or coefficients) to test the performance in $H_{train}$. The test scores are computed as $H_{test}=W_{train}^{-1}X_{test}$.\n",
        "\n",
        "**Question**\n",
        "\n",
        "\n",
        "1.   (IMP+IMH) Plot the basis images and compare them with respect to the basis images obtained using PCA and ICA. What can you say ?\n",
        "2.   (IMP+IMH) What about the performances of NNMF, i.e. computational time and classification accuracy ? Is it better or worse than the other methods ? Why ?\n",
        "3.   (IMP) Implement your own implementation in `NNMFLecture` following the lecture slides. Complete the missing lines (`XXXXXXXXXX`) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IR9Px3TNbhTW"
      },
      "source": [
        "# NNMF (scikit-learn implementation)\n",
        "Ncomponents=100 # choose a number of componenents (r in the slides)\n",
        "model = NMF(init='random', solver='mu', n_components=Ncomponents, tol=1e-3,max_iter=300, random_state=0)\n",
        "WtrainNNMF = model.fit_transform(Xtrain.T)\n",
        "HtrainNNMF = model.components_\n",
        "\n",
        "plotFaces(WtrainNNMF.T,r,c,ncol=2,indeces=np.arange(0,10,1),title='NNMF-faces') \n",
        "\n",
        "Htest_nnmf = np.dot(LA.pinv(WtrainNNMF),Xtest.T)\n",
        "\n",
        "print('NNMF uses ', Ncomponents, ' features')\n",
        "\n",
        "# Score\n",
        "NN.fit(HtrainNNMF.T,Id_Train.ravel())\n",
        "print('Percentage of correct answer using NNMF is ', NN.score(Htest_nnmf.T,Id_Test))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0F4jrirg4Qq"
      },
      "source": [
        "# For IMP\n",
        "def NNMFLecture(X,r=None,N_Iter=1000,tolerance=1e-3,plot_evolution=1):\n",
        "    '''\n",
        "    Inputs: \n",
        "    %           X: is a [dxN] matrix. Every column (x) is an observation and every\n",
        "    %           row consists of features.\n",
        "    %\n",
        "    %           r: size of the matrices W and H\n",
        "    %\n",
        "    %           (Optional) N_Iter: maximum number of iterations\n",
        "    %\n",
        "    %           (Optional) tolerance: convergence criteria threshold\n",
        "    %\n",
        "    %           (Optional) plot_evolution: plot evolution convergence criteria\n",
        "    %\n",
        "    % Outputs:\n",
        "    %           W: is a [d x r] matrix containing the basis images in its\n",
        "    %           columns\n",
        "    %           \n",
        "    %           H: is a [r x N] matrix containing the loadings (h) in its columns\n",
        "    %           of the linear combination: x=Wh \n",
        "    %\n",
        "  '''\n",
        "    if r is None:\n",
        "        r=X.shape[0]\n",
        "        \n",
        "    # Test for positive values\n",
        "    if np.min(X) < 0:\n",
        "        raise NameError('Input matrix X has negative values !')      \n",
        "\n",
        "    # Size\n",
        "    d,N=X.shape\n",
        "   \n",
        "    # Initialization\n",
        "    W=np.random.rand(d,r)\n",
        "    H=np.random.rand(r,N)     \n",
        "    \n",
        "    # parameters for convergence\n",
        "    k = 0\n",
        "    delta = np.inf\n",
        "    eps=np.finfo(float).eps\n",
        "    evolutionDelta=[]\n",
        " \n",
        "    while delta > tolerance and k < N_Iter:\n",
        "        \n",
        "        # multiplicative method      \n",
        "        for i in range(20):\n",
        "            W = np.divide(XXXXXXXX + eps)           \n",
        "            \n",
        "        H = np.divide(XXXXXXXXXX + eps)\n",
        "\n",
        "        # Convergence indices\n",
        "        k = k + 1           \n",
        "        diff=X-np.dot(W,H)     \n",
        "        delta = LA.norm(diff,'fro') / LA.norm(X,'fro') # sqrt(trace(diff'*diff)) / sqrt(trace(X'*X))\n",
        "        evolutionDelta.append(delta)\n",
        "        \n",
        "        if k==1 or k%100==0:\n",
        "            print('Iteration NNMF number ', k, ' out of ', N_Iter , ', delta = ', delta, ', error (norm delta): ', LA.norm(diff))\n",
        "     \n",
        "    if k==N_Iter:\n",
        "        print('Maximum number of iterations reached ! delta = ', delta)\n",
        "    else:\n",
        "        print('Convergence achieved ( delta = ', delta, ') in ', k, ' iterations')\n",
        "    \n",
        "    if plot_evolution==1:\n",
        "        plt.figure(figsize=(6, 6))\n",
        "        plt.plot(range(k),evolutionDelta,'bx--', linewidth=4, markersize=12)  \n",
        "        plt.title('Evolution of error - NNMF')\n",
        "        plt.show()\n",
        "    \n",
        "    return W,H"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KHXEwNMkA2N"
      },
      "source": [
        "# For IMP\n",
        "# NNMF (test your own implementation)\n",
        "XXXXXXXXXXX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOOIrFwqrvWF"
      },
      "source": [
        "Here, we check that using the original data for ICA is definitely too long. In scikit-learn we directly select the number of components $p$. However, results are less satisfactory than using PCA as before or too long."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMaOsHhIejvb"
      },
      "source": [
        "# Shuffle data randomly and use only 300 components (p=300)\n",
        "indeces=np.arange(data.shape[0]) # Integers from 0 to N-1\n",
        "np.random.shuffle(indeces)\n",
        "sample=data[indeces,:]\n",
        "\n",
        "#first architecture (scikit-learn implementation)\n",
        "ICA= FastICA(whiten=True, fun='exp', max_iter=30000, algorithm='parallel', tol=1e-4, random_state=1, n_components=300)\n",
        "Yica=ICA.fit_transform(sample)\n",
        "Sica=Yica.T\n",
        "Aica=ICA.mixing_\n",
        "Wica=ICA.components_\n",
        "\n",
        "# Plot the Eigenfaces\n",
        "plotFaces(Sica,r,c,ncol=2,indeces=np.arange(0,10,1),title='ICA basis images (first architecture)')  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lu6Yp87h2rU"
      },
      "source": [
        "#second architecture (scikit-learn implementation)\n",
        "ICA= FastICA(whiten=True, fun='exp', max_iter=30000, algorithm='parallel', tol=1e-4, random_state=1, n_components=300)\n",
        "Yica=ICA.fit_transform(data.T)\n",
        "Sica=Yica.T\n",
        "Aica=ICA.mixing_\n",
        "Wica=ICA.components_\n",
        "\n",
        "# Plot the Eigenfaces\n",
        "plotFaces(Aica.T,r,c,ncol=2,indeces=np.arange(0,10,1),title='ICA basis images (second architecture)')  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8IMebVawqFO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}