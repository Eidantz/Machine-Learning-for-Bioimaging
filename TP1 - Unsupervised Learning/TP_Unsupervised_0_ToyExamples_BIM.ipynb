{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "name": "TP-Unsupervised-0-ToyExamples-BIM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQdPnbk6EeCs"
      },
      "source": [
        "## Practical Session - Unsupervised Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqlB0cJvEeCu"
      },
      "source": [
        "This Practical session is about unsupervised learning. We will use the dimensionality reduction and clustering techniques presented this morning to analyze toy examples, recognize faces and segment skin lesion images.\n",
        "\n",
        "Please answer the questions and complete the code where you see (`XXXXXXXXXX`). All questions are mandatory for IMP only some are mandatory for IMH.\n",
        "\n",
        "You have two weeks (26/11) to update the three jupyter-notebooks to the Moodle under the section *Reports-TP* **AS A SINGLE ZIP FILE**. You can answer in French or English. The deadline is 23:59 of the 26th of November. I remind you that the report is mandatory and evaluated. \n",
        "\n",
        "**All reports uploaded after the deadline will not be evaluated, namely grade equal to 0**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBkBY3QIEeCv"
      },
      "source": [
        "In this jupyter notebook, you can play with the toy examples shown during the lecture. \n",
        "\n",
        "First let's load the functions we will use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ay0njaQEeCw"
      },
      "source": [
        "import numpy as np\n",
        "import numpy.matlib\n",
        "import matplotlib.pyplot as plt\n",
        "plt.close('all')\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.decomposition import KernelPCA\n",
        "from sklearn.decomposition import FastICA\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "from scipy import linalg as LA\n",
        "from scipy.stats import ortho_group"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usdOLazgEeC3"
      },
      "source": [
        "The next three functions are used to create the data and plot the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmr-d5j3EeC3"
      },
      "source": [
        "def generate_scenario(scenario=3, n_samples0 = 100, n_samples1 = 30):\n",
        "\n",
        "    y = np.concatenate((np.zeros([n_samples0,1]) , np.ones([n_samples1,1])) , axis=0)\n",
        "\n",
        "    if scenario == 1: \n",
        "        # Separate Gaussian\n",
        "        mean0 = [2, 3]\n",
        "        mean1 = [12, 14]\n",
        "        cov0 = [[1, 1.5], [1.5 ,3]]\n",
        "        cov1 = 2 ** 2 * np.eye(2)\n",
        "        X0 = np.random.multivariate_normal(mean0, cov0, n_samples0, check_valid='raise')\n",
        "        X1 = np.random.multivariate_normal(mean1, cov1, n_samples1, check_valid='raise')\n",
        "        \n",
        "    elif scenario == 2:\n",
        "        # Overlapping Gaussian\n",
        "        mean0 = [2, 3]\n",
        "        mean1 = [5, 7]\n",
        "        cov0 = [[1, 1.5], [1.5 ,3]]\n",
        "        cov1 = [[2, 3], [3 ,6]]\n",
        "        X0 = np.random.multivariate_normal(mean0, cov0, n_samples0, check_valid='raise')\n",
        "        X1 = np.random.multivariate_normal(mean1, cov1, n_samples1, check_valid='raise')\n",
        "        \n",
        "        \n",
        "    elif scenario == 3:\n",
        "        # Overlapping Gaussian\n",
        "        mean0 = [0, 0]\n",
        "        mean1 = [0, 0]\n",
        "        cov0 = [[50, 4], [4, 2]]\n",
        "        cov1 = [[2, 0], [0 ,50]]\n",
        "        X0 = np.random.multivariate_normal(mean0, cov0, n_samples0, check_valid='raise')\n",
        "        X1 = np.random.multivariate_normal(mean1, cov1, n_samples1, check_valid='raise')\n",
        "        \n",
        "        \n",
        "    elif scenario == 4:\n",
        "        # Circles\n",
        "        # 1 circle\n",
        "        angle0=np.linspace(0, 2 * np.pi, n_samples0);\n",
        "        X0=np.vstack((8*np.cos(angle0) , 8*np.sin(angle0))).T\n",
        "        \n",
        "        # 2 circle\n",
        "        angle1=np.linspace(0, 2 * np.pi, n_samples1);\n",
        "        X1=np.vstack((2*np.cos(angle1) , 2*np.sin(angle1))).T\n",
        "\n",
        "    return X0,X1,y\n",
        "\n",
        "def plotResults(X=None,U=None,Y=None,const=1,title=''):\n",
        "    \n",
        "    N0=np.sum(y==0)\n",
        "    N1=np.sum(y==1)\n",
        "    \n",
        "    fig=plt.figure(figsize=(17, 6))\n",
        "    \n",
        "    ax  = fig.add_subplot(1, 3, 1)\n",
        "    plt.scatter(X0[:,0],X0[:,1],c='r', label='Class 0')\n",
        "    plt.scatter(X1[:,0],X1[:,1],c='b', label='Class 1')\n",
        "    if U is not None:\n",
        "        average=X.mean(axis=0)\n",
        "        sd=LA.norm(X.std(axis=0))\n",
        "        u0=U[:,0]*const*sd;\n",
        "        u1=U[:,1]*const*sd;\n",
        "        plt.plot([average[0]-u0[0], average[0]+u0[0]],[average[1]-u0[1], average[1]+u0[1]], c='g',linewidth=4, label='C 1' )\n",
        "        plt.plot([average[0]-u1[0], average[0]+u1[0]],[average[1]-u1[1], average[1]+u1[1]], c='k',linewidth=4, label='C 2' )\n",
        "        plt.title('Original data and components')\n",
        "    else:\n",
        "        plt.title('Original data')\n",
        "    plt.legend()\n",
        "    \n",
        "    ax  = fig.add_subplot(1, 3, 2)\n",
        "    plt.scatter(Y[np.where(y == 0)[0],0], np.zeros((N0,1)), c='r', s=3, marker='o', label='Class 0')\n",
        "    plt.scatter(Y[np.where(y == 1)[0],0], np.zeros((N1,1)), c='b', s=3, marker='x', label='Class 1')\n",
        "    ax.set_title(title + '\\n Scores on 1st component')\n",
        "    \n",
        "    ax  = fig.add_subplot(1, 3, 3)\n",
        "    plt.scatter(Y[np.where(y == 0)[0],1], np.zeros((N0,1)), c='r', s=3, marker='o', label='Class 0')\n",
        "    plt.scatter(Y[np.where(y == 1)[0],1], np.zeros((N1,1)), c='b', s=3, marker='x', label='Class 1')\n",
        "    plt.legend()\n",
        "    plt.title('Scores on 2nd component')\n",
        "    \n",
        "    plt.show()\n",
        "    \n",
        "    \n",
        "def frontiere(model, X, y, step=50):\n",
        "\n",
        "    labels = np.unique(y)\n",
        " \n",
        "    min_tot = np.min(X)\n",
        "    max_tot = np.max(X)\n",
        "    delta = (max_tot - min_tot) / step\n",
        "    xx, yy = np.meshgrid(np.arange(min_tot, max_tot, delta),\n",
        "                         np.arange(min_tot, max_tot, delta))\n",
        "    z = np.array( model.predict(np.c_[xx.ravel(), yy.ravel() ]) )\n",
        "    z = z.reshape(xx.shape)\n",
        "   \n",
        "    plt.imshow(z, origin='lower', extent=[min_tot, max_tot, min_tot, max_tot],\n",
        "               interpolation=\"mitchell\", cmap='RdBu')\n",
        "    \n",
        "    cbar = plt.colorbar(ticks=labels)\n",
        "    cbar.ax.set_yticklabels(labels)\n",
        "\n",
        "    plt.scatter(X[np.where(yKmeans == 0)[0],0],X[np.where(yKmeans == 0)[0],1],c='r', label='Predicted class 0')\n",
        "    plt.scatter(X[np.where(yKmeans == 1)[0],0],X[np.where(yKmeans == 1)[0],1],c='b', label='Predicted class 1') \n",
        "    \n",
        "    plt.ylim([min_tot, max_tot])\n",
        "    plt.xlim([min_tot, max_tot])\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNlIyjsHEeC7"
      },
      "source": [
        "Now,let's create the data we will use.\n",
        "Try the 4 different scenarios by simply varying the `scenarioIndex` value between 1 and 4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6Rpg84vEeC9"
      },
      "source": [
        "## Choose the scenarioIndex (value between 1 and 4)\n",
        "scenarioIndex = 1\n",
        "##\n",
        "\n",
        "X0,X1,y = generate_scenario(scenario=scenarioIndex, n_samples0 = 350, n_samples1 = 350)\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.scatter(X0[:,0],X0[:,1],c='r', label='Class 0')\n",
        "plt.scatter(X1[:,0],X1[:,1],c='b', label='Class 1')\n",
        "plt.title('Original data')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iavcvJKmEeDC"
      },
      "source": [
        "It's time to use the methods seen this morning. \n",
        "\n",
        "As you can see, we have generated two populations (class 0 and class 1). We concatenate them as a single matrix *X* which will be the input for all methods. In this way, the methods will be unaware of the class of the observations (unsupervised) and we will test whether the methods are appropriate for the analysed scenario and if they are able to use less dimensions to correctly distinguish the two classes. \n",
        "\n",
        "Let's start with PCA. \n",
        "\n",
        "**Question:**\n",
        "1. (IMP + IMH) Use PCA with the different 4 scenarios and comment the results. When does PCA work well ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xRZeINdEeDD"
      },
      "source": [
        "X=np.concatenate((X0,X1),axis=0)\n",
        "\n",
        "## PCA\n",
        "pca = PCA(random_state=1) # by fixing the random_state we are sure that results are always the same\n",
        "Ypca=pca.fit_transform(X)\n",
        "U=pca.components_.T # we want PC on columns\n",
        "D = (pca.singular_values_)**2/(X.shape[0]-1) # computation of the eigenvalues\n",
        "\n",
        "print('The variance explained by the two first modes is respectively: ', pca.explained_variance_ratio_)\n",
        "plotResults(X,U,Ypca,const=1,title='PCA')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwF92npaEeDL"
      },
      "source": [
        "**Question:**\n",
        "1. (IMP) Instead than using the scikit-learn implementation, implement one on your own ! Complete the code where you see **XXXXXXXXXXXXXX**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "km2F1TyqEeDM"
      },
      "source": [
        "def pcaLecture(X):\n",
        "    ''' \n",
        "    Inputs: \n",
        "            X: is a [Nxd] matrix. Every row is an observation and every\n",
        "              column consists of features.\n",
        "    Outputs:\n",
        "            Y: is a [Nxd] matrix representing the scores, namely the \n",
        "            coordinates of X onto the new basis given by the eigenvactors U\n",
        "            of the covariance matrix of X. Columns are the principal components.\n",
        "               \n",
        "            U: columns are Eigenvectors (sorted from the greatest to the lowest eigenvalue)\n",
        "    \n",
        "            D: Eigenvalues (sorted from the greatest to the lowest eigenvalue)\n",
        "               \n",
        "            var_explained: percentage of the original variability explained\n",
        "            by each principal component.\n",
        "    '''\n",
        "    \n",
        "    N=X.shape[0]\n",
        "    Xc=XXXXXXXX # centering\n",
        "    D2, Uh = LA.svd(Xc)[1:3] # computation of eigenvectors and eigenvalues using SVD\n",
        "    U=Uh.T\n",
        "    #De, Ue = LA.eig(np.matmul(Xc.T,Xc)/(N-1))    \n",
        "    Y=XXXXXXX # computation of the scores, use np.dot()\n",
        "    D=D2**2/(N-1) # computation of the eigenvalues\n",
        "    tot=np.sum(D)\n",
        "    var_explained = D*100/tot # computation of explained variance\n",
        "    return Y,U,D,var_explained"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYrpvPUiEeDQ"
      },
      "source": [
        "**Question:**\n",
        "1. (IMP ) Test your own implementation and check whether the results are the same of the scikit-learn implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlAFSB1uEeDQ"
      },
      "source": [
        "YpcaLec,UpcaLec,DpcaLec,var_explainedPcaLec=pcaLecture(X)\n",
        "plotResults(X,UpcaLec,YpcaLec,const=1,title='PCA Lecture')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duBef8WYEeDV"
      },
      "source": [
        "Let's use Kernel-PCA with the rbf kernel (you can also test other kernels if you want).\n",
        "\n",
        "**Question:**\n",
        "1. (IMP + IMH) Use Kernel-PCA with the different 4 scenarios and comment the results. When does K-PCA work well ? Why ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UInYXsXrEeDW"
      },
      "source": [
        "# Kernel-PCA\n",
        "gamma=2\n",
        "Kpca = KernelPCA(kernel='rbf', gamma=gamma, random_state=1)\n",
        "YKpca=Kpca.fit_transform(X)\n",
        "DKpca=Kpca.lambdas_\n",
        "AKpca=Kpca.alphas_\n",
        "\n",
        "plotResults(X=X,Y=YKpca,const=1,title='KPCA')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDFuV2pEEeDZ"
      },
      "source": [
        "**Question:**\n",
        "1. (IMP) Instead than using the scikit-learn implementation, implement one on your own ! Complete the code where you see **XXXXXXXXXXXXXX**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3ulQzkkEeDZ"
      },
      "source": [
        "def KpcaGaussianLecture(X,gamma):\n",
        "    '''  \n",
        "    Inputs: \n",
        "            X: is a [Nxd] matrix. Every row is an observation and every\n",
        "            column is a feature.\n",
        " \n",
        "    Outputs:\n",
        "            Y: is a [Nxd] matrix representing the scores, namely the \n",
        "            coordinates of \\phi(X) onto the new basis given by the eigenvactors \n",
        "            of the covariance matrix of \\phi(X). Columns are the principal components.\n",
        "\n",
        "            An: columns are Eigenvectors normalised (sorted from the greatest\n",
        "            to the lowest eigenvalue)\n",
        "     \n",
        "            D: Eigenvalues (sorted from the greatest to the lowest eigenvalue)\n",
        "\n",
        "            var_explained: percentage of the original variability explained\n",
        "            by each principal component.\n",
        "\n",
        "    '''\n",
        "\n",
        "    # kernel matrix\n",
        "    def kernel_matrix(X,gamma):\n",
        "      N=X.shape[0]\n",
        "      InnerX = np.dot(X,X.T)\n",
        "      temp1=np.sum(X**2,axis=1).reshape((N,1))\n",
        "      temp2=np.sum(X**2,axis=1).reshape((1,N))\n",
        "      Norm1 = np.repeat(temp1,N,axis=1)\n",
        "      Norm2 = np.repeat(temp2,N,axis=0) \n",
        "      Norm = Norm1+Norm2-2*InnerX;\n",
        "      Norm[Norm<1e-10]=0;    \n",
        "      K=np.exp(-Norm/(2*gamma**2))\n",
        "      return K\n",
        "       \n",
        "    N=X.shape[0]\n",
        "\n",
        "    K=kernel_matrix(X,gamma)\n",
        "    \n",
        "    oneN=np.ones((N,N))/N;\n",
        "    Kc=XXXXXXXXXXXXXXX # center kernel matrix\n",
        "\n",
        "    # eigenvalue analysis\n",
        "    D,A=LA.eigh(Kc)     \n",
        "    idx = D.argsort()[::-1]  # reverse order to make 'descend' \n",
        "    D = np.real(D[idx])\n",
        "    D[D<0]=1e-18 # make negative eigenvalues positive (and almost 0)\n",
        "    A = np.real(A[:,idx])\n",
        "\n",
        "    # variance explained\n",
        "    tot=np.sum(D)\n",
        "    var_explained = D/tot # computation of explained variance\n",
        "\n",
        "    # Normalisation eigenvectors\n",
        "    # Norm of every eigenvector is 1, we want it to be 1/sqrt(N*eig)\n",
        "    \n",
        "    An=np.copy(A)\n",
        "    for i in range(N):      \n",
        "        An[:,i]=np.dot(A[:,i],(1/np.sqrt((N-1)*D[i])) )          \n",
        "           \n",
        "    Y=XXXXXXXX # computation of the scores, use np.dot()  \n",
        "    \n",
        "    return Y,An,D,var_explained"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYg6TDvVEeDd"
      },
      "source": [
        "**Question:**\n",
        "1. (IMP) Test your own implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JALc6EMeEeDe"
      },
      "source": [
        "YKpcaLec, AnKpcaLec, DKpcaLec, var_explainedKpca = KpcaGaussianLecture(X,gamma)\n",
        "plotResults(X=X,Y=YKpcaLec,const=1,title='KPCA Lecture')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qgw-lR7CEeDh"
      },
      "source": [
        "Now, test ICA.\n",
        "\n",
        "**Question:**\n",
        "1. (IMP + IMH) Use ICA with the different 4 scenarios and comment the results. When it works better than PCA ? Why ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CwuLJyJEeDh"
      },
      "source": [
        "## ICA\n",
        "ICA= FastICA(whiten=True, fun='exp', max_iter=20000, tol=0.00001, random_state=1)\n",
        "Yica=ICA.fit_transform(X)\n",
        "Wica=ICA.mixing_\n",
        "plotResults(X=X,U=Wica,Y=Yica,const=0.01,title='ICA')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZ7Y2uNTEeDm"
      },
      "source": [
        "**Question:**\n",
        "1. (IMP) Instead than using the scikit-learn implementation, implement one on your own !\n",
        "Complete the code where you see **XXXXXXXXXXXXXX**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LKyxk-0EeDn"
      },
      "source": [
        "def FastICALecture(X,N_Iter=3000,tol=1e-5,plot_evolution=0):\n",
        "    '''\n",
        "    Inputs: \n",
        "                   X: is a [d x N] matrix. Every column is an observation \n",
        "                   and every row is a feature.       \n",
        "    \n",
        "                   (Optional) N_Iter: maximum number of iterations\n",
        "    \n",
        "                   (Optional) delta: convergence criteria threshold\n",
        "    \n",
        "                   (Optional) plot_evolution: plot evolution of error\n",
        "                   \n",
        "     Outputs:      \n",
        "                   S: [d x N] matrix.  Each column is an independent component \n",
        "                   of the centred and whitened input data X              \n",
        "                   \n",
        "                   W: [d x d] matrix. It is the demixing matrix. S = W*Xcw \n",
        "    \n",
        "    '''\n",
        "    \n",
        "    # First derivative of G   \n",
        "    def g(t):\n",
        "        res = t * np.exp(-(t**2)/2)\n",
        "        return res\n",
        "    \n",
        "    # Second derivative of G  \n",
        "    def gp(t):\n",
        "        res = (1 - t**2) * np.exp(-(t**2)/2)\n",
        "        return res\n",
        "    \n",
        "    # Size of X\n",
        "    d,N=X.shape \n",
        "        \n",
        "    # Compute sample mean   \n",
        "    mu = X.mean(axis=1, keepdims=True)\n",
        "    \n",
        "    # Center data    \n",
        "    Xc=XXXXXXXXXX\n",
        "    \n",
        "    # Compute covariance matrix    \n",
        "    C=XXXXXXXXXXX\n",
        "    \n",
        "    # Whiten data\n",
        "    Xcw=np.dot(LA.inv(LA.sqrtm(C)),Xc)\n",
        "    \n",
        "    # check if are whitened\n",
        "    if np.sum(np.eye(d) - np.abs(np.cov(Xcw)))>1e-10:\n",
        "        raise NameError('Your whitening transformation does not work...')\n",
        "    \n",
        "    # Initialize W\n",
        "    W = ortho_group.rvs(d) # random orthogonal matrix \n",
        "    \n",
        "    # delta evolution\n",
        "    k = 0\n",
        "    delta = np.inf\n",
        "    evolutionDelta=[]\n",
        "    \n",
        "    while delta > tol and k < N_Iter:\n",
        "    \n",
        "        k = k + 1\n",
        "        W_old = np.copy(W)\n",
        "        \n",
        "        Wp = np.dot(g(np.dot(W,Xcw)),Xcw.T) - np.dot(np.diagflat(np.dot(gp(np.dot(W,Xcw)),np.ones((N,1)))),W)  \n",
        "        W = np.dot(LA.inv(LA.sqrtm(np.dot(Wp,Wp.T))),Wp) # W*W'=I\n",
        "        if np.sum(np.eye(d)-np.abs(np.dot(W,W.T)))>1e-10:\n",
        "            raise NameError('W should be an orthogonal matrix. Check the computations')\n",
        "                 \n",
        "        delta = 1-np.min(np.abs(np.diag(np.dot(W.T,W_old))))\n",
        "        evolutionDelta.append(delta)\n",
        "        \n",
        "        if k==1 or k%100==0:\n",
        "            print('Iteration ICA number ', k, ' out of ', N_Iter , ', delta = ', delta)\n",
        "     \n",
        "        \n",
        "    if k==N_Iter:\n",
        "        print('Maximum number of iterations reached ! delta = ', delta)\n",
        "    else:\n",
        "        print('Convergence achieved ( delta = ', delta, ') in ', k, ' iterations')\n",
        "\n",
        "\n",
        "    # Independent components\n",
        "    S = XXXXXXXXXXXXX\n",
        "            \n",
        "    if plot_evolution==1:\n",
        "        plt.figure(figsize=(6, 6))\n",
        "        plt.plot(range(k),evolutionDelta,'bx--', linewidth=4, markersize=12)  \n",
        "        plt.title('Evolution of error - ICA')\n",
        "        plt.show()\n",
        "       \n",
        "    return S,W"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tn1VilB8EeDq"
      },
      "source": [
        "**Question:**\n",
        "1. (IMP) Test your own implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Vaex78wEeDr"
      },
      "source": [
        "SicaLec,WicaLec = FastICALecture(X.T,N_Iter=3000,tol=1e-5,plot_evolution=1)\n",
        "plotResults(X=X, U=WicaLec.T, Y=SicaLec.T, const=1, title='ICA Lecture')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a60dmwTWEeDw"
      },
      "source": [
        "With a different perspective, we could also use K-means. As before, we will use it on X and we will check whether it can well separate the two classes. \n",
        "\n",
        "\n",
        "**Question:**\n",
        "1. (IMP + IMH) Does it work well in all scenarios ? Why ? Is it always easy to define the correct number of clusters ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRxKO8tZEeDx"
      },
      "source": [
        "## K-means\n",
        "Ncluster= 2 # choose a number of clusters\n",
        "kmeans=KMeans(n_clusters=Ncluster) \n",
        "yKmeans=kmeans.fit_predict(X)\n",
        "\n",
        "plt.figure(figsize=(17, 6))\n",
        "plt.subplot(131)\n",
        "plt.scatter(X[np.where(y == 0)[0],0],X[np.where(y == 0)[0],1],c='r', label='Class 0')\n",
        "plt.scatter(X[np.where(y == 1)[0],0],X[np.where(y == 1)[0],1],c='b', label='Class 1')\n",
        "plt.title('Original data')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(132)\n",
        "plt.scatter(X[np.where(yKmeans == 0)[0],0],X[np.where(yKmeans == 0)[0],1],c='r', label='Predicted class 0')\n",
        "plt.scatter(X[np.where(yKmeans == 1)[0],0],X[np.where(yKmeans == 1)[0],1],c='b', label='Predicted class 1')\n",
        "plt.title('K-Means')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(133)\n",
        "frontiere(kmeans, X, y, step=50)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}